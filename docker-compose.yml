version: "3.9"

# This docker compose uses profiles - see https://docs.docker.com/compose/profiles/

# 
# DOCKER IMAGES
#

# 13-3.1 is the first pg version with FORCE_SSL support
# needed to force clients to use SSL
x-postgres-image: &postgres-image kartoza/postgis:13-3.1
x-osm-update-image: &osm-update-image kartoza/docker-osm:osmupdate-latest
x-imposm-image: &imposm-image kartoza/docker-osm:imposm-latest
x-osmenrich-image: &osm-enrich-image kartoza/docker-osm:osmenrich-latest
# Special build of the kartoza geoserver image which includes the 
# plugins etc. GeoNode needs to communicate with GeoServer
x-geoserver-image: &geoserver-image kartoza/geoserver:2.18.2-geonode
x-mergin-db-sync-image: &mergin-db-sync-image lutraconsulting/mergin-db-sync
x-nginx-image: &nginx-image nginx
x-certbot-image: &certbot-image certbot/certbot
x-qgis-server-image: &qgis-server-image openquake/qgis-server:stable
x-mapproxy-image: &mapproxy-image kartoza/mapproxy
x-postgrest-image: &postgrest-image postgrest/postgrest
x-swagger-image: &swagger-image swaggerapi/swagger-ui
x-scp-image: &scp-image quay.io/lkiesow/docker-scp
x-odm-image: &odm-image opendronemap/odm
x-gdal-image: &gdal-image geodata/gdal
x-mergin-client-image: &mergin-client-image kartoza/mergin-client:latest
x-lizmap-image: &lizmap-image 3liz/lizmap-web-client:3.4
x-geonode-image: &geonode-image geonode/geonode:3.1

# 
# VOLUMES
#

# Generally we are using docker volumes for all volumes
# except conf files which are host mounted volumes out
# of our git repo.
# 

volumes:
  import_done:
  import_queue:
  postgis_data:
  geoserver_data:
  mapproxy_data:
  mergin_sync_data:
  osm_import_done:
  osm_import_queue:
  osm_cache:
  odm_cutline:
  odm_datasets:
  redis:
  cache:
  #
  # For uploading stuff via scp
  #
  scp_etc:
  # Mounted in SCP and to geoserver for a file store
  scp_geoserver_data:
  # Mounted in SCP and to QGIS Server for serving QGIS projects
  qgis_projects:
  # Mounted in SCP and to QGIS Server for serving QGIS fonts
  scp_qgis_fonts:
  # Mounted in SCP and to QGIS Server for serving QGIS svgs
  scp_qgis_svg:
  # Mounted in SCP and to Hugo for rendering static pages
  scp_hugo_data:
  # Mounted in SCP and to ODM for processing uploaded images
  scp_odm_data:
  # Mounted in SCP only for data sharing
  scp_general_data:
  # Lizmap related volumes
  lizmap-theme-config:
  lizmap-config:
  lizmap-db:
  lizmap-www:
  lizmap-log:
  # GeoNode related volumes
  geonode-static-files:
  geonode-backup-restore:
  geonode-data:
  geonode-tmp:

networks:
  seafile-net:

#
# SERVICES
#

services:
 
  ############################################################
  #
  # First we define all the services in the PRODUCTION profile
  # These are the database, QGIS Server, and all reverse proxied
  # services in nginx such as Postgrest, Swagger, GeoServer etc.
  #
  ############################################################

  db:
    image: *postgres-image
    profiles: ["all","db", "production","nginx","postgres","osm","geoserver","qgis-server", "mergin", "dev"]
    volumes:
      # This will create two users on startup if they dont already exist
      # One user for the geonode instance to connect to the geonode database
      # for storing application state.
      # One user for the geonode instance to connect to the geonode_data database
      # to store uploaded geospatial data.
      # Both need read / write access for the stack to work
      - ./pg_conf/init-users.sh:/docker-entrypoint-initdb.d/init-users.sh
      # Persistent storage of the database cluster 
      - postgis_data:/var/lib/postgresql
      # So we can use SSL certs for pg too
      # Currently NOT WORKING - eventually we can set this and then
      # Enable full CA based encryption
      - ./certbot/certbot/conf:/etc/letsencrypt
    env_file: .env
    environment:
      # You can define multiple databases by providing a comma
      # separated list here.
      - POSTGRES_DB=gis,${GEONODE_DATABASE},${GEONODE_GEODATABASE}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASS=${POSTGRES_PASSWORD}
      # Read the notes at
      # https://github.com/kartoza/docker-postgis#postgres-ssl-setup
      # Certificate from letsencrypt doesn't currently work
      #- SSL_CERT_FILE=/etc/letsencrypt/live/${DOMAIN}/cert.pem
      #- SSL_KEY_FILE=/etc/letsencrypt/live/${DOMAIN}/privkey.pem
      #- SSL_CA_FILE=/your/own/ssl_ca_file.pem
      # Force clients to connect with SSL - will add hostssl
      # instead of host to lines in pg_hba.conf
      - FORCE_SSL=TRUE
      # When connecting using your client, set SSL mode to 'Require'
      # If you want force using a certificate, use PASSWORD_AUTHENTICATION=cert
      # But then you also need to deploy certificates (again see the docs above)
      - ALLOW_IP_RANGE=0.0.0.0/0
      - PASSWORD_AUTHENTICATION=scram-sha-256
      # You can pass as many extensions as you need.
      - POSTGRES_MULTIPLE_EXTENSIONS=postgis,hstore,postgis_topology,postgis_raster,pgrouting
      # Some extensions need to be registered in the postgresql.conf as
      # shared_preload_libraries. pg_cron should always be added because the
      # extension is installed with the image.
      - SHARED_PRELOAD_LIBRARIES=pg_cron 
      - DEFAULT_ENCODING="UTF8"
      - DEFAULT_COLLATION="en_US.UTF-8"
      - DEFAULT_CTYPE="en_US.UTF-8"
      - POSTGRES_TEMPLATE_EXTENSIONS=true


    networks:
     - os-gis-stack
    ports:
      - ${POSTGRES_PUBLIC_PORT}:5432 
    restart: unless-stopped
    healthcheck:
        test: "exit 0"

  lizmap:
    # Need to resolve an issue with lizmap home still......
    image: *lizmap-image
    profiles: ["all","production","nginx","lizmap", "dev"]
    command: 
      - php-fpm
    env_file: .env
    environment:
      LIZMAP_WPS_URL: https://${DOMAIN}/lizmap/ # see nginx.conf
      LIZMAP_CACHESTORAGETYPE: redis   
      LIZMAP_CACHEREDISDB: '1'
      LIZMAP_USER: '1010'
      LIZMAP_WMSSERVERURL: https://${DOMAIN}/lizmap/ows/
      LIZMAP_CACHEREDISHOST: redis
      LIZMAP_HOME: /srv/lizmap/
    volumes:
      - qgis_projects:/srv/projects
      - lizmap-theme-config:/www/lizmap/var/lizmap-theme-config
      - lizmap-config:/www/lizmap/var/config
      - lizmap-db:/www/lizmap/var/db
      - lizmap-www:/www/lizmap/www
      - lizmap-log:/www/lizmap/var/log
    networks:
      - os-gis-stack

  # Geoserver backend specifically supporting GeoNode
  # that includes the GeoNode plugins
  # deployed in the Kartoza GeoServer image
  geoserver:
    image: *geoserver-image
    profiles: ["all","production","nginx","geoserver", "dev"]
    healthcheck:
      test: "curl --fail --silent --write-out 'HTTP CODE : %{http_code}\n' --output /dev/null http://127.0.0.1:8080/geoserver/rest/workspaces/geonode.html"
      interval: 60s
      timeout: 10s
      retries: 1 
      start_period: 60s
    depends_on:
        - db    
    env_file:
      # Editable configs (will be edited by make configure or by user)
      - .env
      # Non editible configs in this one
      - .env-geonode-geoserver
    environment:
      - OAUTH2_CLIENT_ID=${OAUTH2_CLIENT_ID}
      - OAUTH2_CLIENT_SECRET=${OAUTH2_CLIENT_SECRET}
      - ADMIN_USERNAME=${GEOSERVER_ADMIN_USER}
      - ADMIN_PASSWORD=${GEOSERVER_ADMIN_PASSWORD}
      - INITIAL_MEMORY=${INITIAL_MEMORY}
      - MAXIMUM_MEMORY=${MAXIMUM_MEMORY}
      - STABLE_EXTENSIONS=importer-plugin
      - HTTPS_HOST=${DOMAIN}
      - HTTPS_PORT=443
      - HTTP_HOST=127.0.0.1
      - HTTP_PORT=80
      - GEOSERVER_DATA_DIR=/opt/geoserver/data_dir
      # See this link for explanation of URL schemes below:
      #  https://github.com/kennethreitz/dj-database-url#url-schema
      # This is the database that geonode stores its content in
      - DATABASE_URL=postgres://${GEONODE_DATABASE_USER}:${GEONODE_DATABASE_PASSWORD}@db:5432/${GEONODE_DATABASE}
      # This is hte database that GeoServer uses of its data for stuff uploaded in geonode
      - GEODATABASE_URL=postgis://${GEONODE_GEODATABASE_USER}:${GEONODE_GEODATABASE_PASSWORD}@db:5432/${GEONODE_GEODATABASE}
    volumes:
      - geonode-static-files:/mnt/volumes/statics
      #- geonode-geoserver-data:/geoserver_data/data
      - geonode-backup-restore:/backup_restore
      - geonode-data:/data
      - geonode-tmp:/tmp      
      # Must create this dir yourself so that it is owned by you
      # This path is specified by the GEOSERVER_DATA_DIR
      - geoserver_data:/opt/geoserver/data_dir
      # Mounted in SCP and to geoserver for serving geoserver data
      # store
      - scp_geoserver_data:/data/scp_geoserver_data
    restart: on-failure
    networks:
      - os-gis-stack    


  # This is only intended to be used during the initial setup of
  # certbot, thereafter use the nginx or nginx-dev services       
  nginx-certbot-init:
    image: *nginx-image
    profiles: ["certbot-init"]
    ports:
      - "80:80"
      - "443:443"
    volumes:
     - ./nginx_certbot_init_conf/nginx.conf:/etc/nginx/nginx.conf
     - ./certbot/certbot/conf:/etc/letsencrypt
     - ./certbot/certbot/www:/var/www/certbot

 
  # Nginx with ssl      
  nginx:
    image: *nginx-image
    profiles: ["all","production","nginx"]
    volumes:
     - ./html:/var/www/html
     - ./nginx_conf/nginx.conf:/etc/nginx/nginx.conf
     - ./certbot/certbot/conf:/etc/letsencrypt
     - ./certbot/certbot/www:/var/www/certbot
     - geonode-static-files:/var/www/geonode-static
     # Mount the mergin db sync data into nginx so we can access
     # images etc.
     # Ideally we should only be mounting the image subfolders to
     # avoid leaking our geopackage etc.
     # To do that the mergin project needs to be set up accordingly
     # - TODO for the future
     - mergin_sync_data:/mergin-data
    # This makes nginx reload its configuration (and certificates)
    # every six hours in the background and launches nginx in the
    # foreground.
    # See
    # https://medium.com/@pentacent/nginx-and-lets-encrypt-with-docker-in-less-than-5-minutes-b4b8a60d3a71
    command: "/bin/sh -c 'while :; do sleep 6h & wait $${!}; nginx -s reload; done & nginx -g \"daemon off;\"'"
    ports:
     - "80:80"
     - "443:443"
    networks:
     - os-gis-stack
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: 200m
        max-file: '10'
    depends_on:
      - qgis-server
      - mapproxy
      - geoserver
      - postgrest
      - swagger
      - geonode

  certbot:
    image: *certbot-image
    profiles: ["all","production","nginx","certbot-init"]
    # This checks every 12 hours if our cert is up for renewal and
    # refreshes it if needed
    # See https://medium.com/@pentacent/nginx-and-lets-encrypt-with-docker-in-less-than-5-minutes-b4b8a60d3a71
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"
    volumes:
     - ./certbot/certbot/conf:/etc/letsencrypt
     - ./certbot/certbot/www:/var/www/certbot

  qgis-server:
    image: *qgis-server-image
    profiles: ["all","production","nginx","qgis-server","dev"]
    #image: "openquake/qgis-server:3"
    env_file: .env
    environment:
      # Do not run the embedded copy of nginx
      SKIP_NGINX: "true"
      # Improve rendering performance
      QGIS_SERVER_PARALLEL_RENDERING: "true"
      QGIS_SERVER_MAX_THREADS: 4
      # Limit the maximum size returned by a GetMap
      QGIS_SERVER_WMS_MAX_HEIGHT: 5000
      QGIS_SERVER_WMS_MAX_WIDTH: 5000
      # Verbose logging
      QGIS_SERVER_LOG_LEVEL: 0
      # Initialize the authentication system
      # creates or uses qgis-auth.db in ~/.qgis3/ or directory
      # defined by QGIS_AUTH_DB_DIR_PATH env variable
      # set the master password as first line of file defined by
      # QGIS_AUTH_PASSWORD_FILE env variable
      # (QGIS_AUTH_PASSWORD_FILE variable removed from environment
      # after accessing)
      # See also volume mounts below
      QGIS_AUTH_DB_DIR_PATH: /tmp/
      QGIS_AUTH_PASSWORD_FILE: /tmp/qgis-auth-pwd.txt
      # For OGC API
      # # Landing page plugin projects directory
      #QGIS_SERVER_LANDING_PAGE_PROJECTS_DIRECTORIES ${QGIS_SERVER_LANDING_PAGE_PROJECTS_DIRECTORIES}
      #QGIS_SERVER_LANDING_PAGE_PROJECTS_PG_CONNECTIONS postgresql://${POSTGRES_USER}:${POSTGRES_PASS}@postgis:5432?sslmode=disable&dbname=${POSTGRES_DBNAME}&schema=public`
    ports:
      - "9993"
    networks:
     - os-gis-stack
    volumes:
     # Data should be mount RO when working
     # with GeoPackages and more than one QGIS container
     # Data dir structure
     # $(pwd)/qgis_projects must have the following structure:
     #
     # data
     # |
     # |-- <project_name>
     #   |-- <project_name>.qgs
     # 
     # The data dir is mounted in the SCP volume
     # too so that you can upload your projects easily
     # using SCP and then have them published
     #
     - qgis_projects:/io/data:ro
     # 
     # qgis_plugins
     # |
     # |-- <plugin_name>
     #   |-- <plugin_code>.py
     #   |-- metadata.txt
     #   |-- __init__.py
     - ./qgis_plugins:/io/plugins
     #  
     # Custom fonts are loaded into /usr/share/fonts. fc-cache is
     # run when container is started.
     - scp_qgis_fonts:/usr/share/fonts
     - scp_qgis_svg:/var/lib/qgis/.local/share/QGIS/QGIS3/profiles/default/svg
     - ./qgis_conf/qgis-auth.db:/tmp/qgis-auth.db
     - ./qgis_conf/qgis-auth-pwd.txt:/tmp/qgis-auth-pwd.txt
     # Working path for the pg_service file if using
     # openquake/qgis-server:stable
     # See https://github.com/gem/oq-qgis-server/issues/54
     # The service file contains two service definitions
     # nginx - used for connecting to the database to load a QGIS
     #         project stored in the database
     #         In that case you cannot yet use qgis-auth.db
     #         database for user/pass credentials
     # smallholding - used for layer definitions. This service has
     #         no user/pass data and 
     #         instead we fetch those credentials from the qgis
     #         authdb
     - ./pg_conf/pg_service.conf:/etc/postgresql-common/pg_service.conf:ro
     # As per the oq QGIS Server docs at
     # https://github.com/gem/oq-qgis-server/blob/master/README.md#postgresql-connection-service-file-optional
     # but wrong! 
     #This works instead for the openquake/qgis-server:3 image
     #- ./pg_conf/pg_service.conf:/etc/pg_service.conf:ro

     # Mount the mergin db sync data into nginx so we can access
     # images etc.
     # Ideally we should only be mounting the image subfolders to
     # avoid leaking our geopackage etc.
     # To do that the mergin project needs to be set up accordingly
     # - TODO for the future
     - mergin_sync_data:/mergin-data     
    depends_on:
      - db
    restart: unless-stopped

  mapproxy:
    image: *mapproxy-image
    profiles: ["all","production","nginx","qgis-server","dev", "mapproxy"]
    env_file: .env
    environment:
      - PRODUCTION=false
      - PROCESSES=4
      - THREADS=10
    user: "1000:10001"
    volumes:
      - mapproxy_data:/mapproxy/
      - ./mapproxy_conf/app.py:/mapproxy/app.py
      - ./mapproxy_conf/seed.yaml:/mapproxy/seed.yaml
      - ./mapproxy_conf/mapproxy.yaml:/mapproxy/mapproxy.yaml
    networks:
     - os-gis-stack

  mapproxy-seed:
    image: *mapproxy-image
    profiles: ["mapproxy-seed"]
    entrypoint: ["mapproxy-seed -c 4 -s /mapproxy/seed.yaml -f /mapproxy/mapproxy.yaml"]
    #command: "mapproxy-seed -c 4 -s /mapproxy/seed.yaml -f /mapproxy/mapproxy.yaml"
    env_file: .env
    environment:
      - PRODUCTION=false
      - PROCESSES=4
      - THREADS=10
    user: "1000:10001"
    volumes:
      - mapproxy_data:/mapproxy/
      - ./mapproxy_conf/app.py:/mapproxy/app.py
      - ./mapproxy_conf/seed.yml:/mapproxy/seed.yml
      - ./mapproxy_conf/mapproxy.yml:/mapproxy/mapproxy.yml
    depends_on:
      - qgis-server
    networks:
      - os-gis-stack     

  postgrest:
    #
    # Used for pushing readings from IoT devices to our database
    #
    image: *postgrest-image
    profiles: ["all","production","postgrest","dev"]
    env_file: .env
    environment:
      PGRST_DB_URI: postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/gis
      PGRST_DB_SCHEMA: ${PGRST_DB_SCHEMA}
      # In production this role should not be the same as the one used for the connection
      # depends_on
      PGRST_DB_ANON_ROLE: ${PGRST_DB_ANON_ROLE}
      PGRST_SERVER_PROXY_URI: ${PGRST_SERVER_PROXY_URI}
      PGRST_OPENAPI_SERVER_PROXY_URI: ${PGRST_OPENAPI_SERVER_PROXY_URI}
      PGRST_SERVER_PORT: ${PGRST_SERVER_PORT}
    links:
      - db:db
    ports:
      - "3000"
    depends_on:
      - db
    networks:
      - os-gis-stack

  swagger:
    image: *swagger-image
    profiles: ["all","production","postgrest","dev"]
    ports:
      - "8080"
    volumes:
      - ./swagger_conf/swagger.json:/swagger.json
    env_file: .env
    environment:
      SWAGGER_JSON: /swagger.json
      API_URL: ${API_URL}       
    depends_on:
      - db
      - postgrest
    networks:
      - os-gis-stack
  scp:
    image: *scp-image
    profiles: ["all","production","scp","dev"]
    ports:
      # This is one of the few cases where we are 
      # exposing a port to the host. Users should
      # be able to use scp to upload qgis projects
      # data files for geoserver etc.
      - "2222:22"
    volumes:
      # Host mounted volumes./mapproxy_conf/mapproxy.yml
      # We need to persist the /etc/ssh folder so that 
      # Docker restart don't create a new host key each time
      # because clients connecting will error out with host
      # key changed errors
      - scp_etc:/etc/ssh
      # One file per user, each file containing list
      # of ssh keys allowed to upload to the data dir
      # Files should be given same name as user
      - ./scp_conf:/conf
      # Mounted in SCP and to geoserver for serving geoserver data
      # store
      - scp_geoserver_data:/data/geoserver_data
      # Mounted in SCP and to QGIS Server for serving QGIS projects
      - qgis_projects:/data/qgis_projects
      # Mounted in SCP and to QGIS Server for serving QGIS fonts
      - scp_qgis_fonts:/data/qgis_fonts
      # Mounted in SCP and to QGIS Server for serving QGIS svgs
      - scp_qgis_svg:/data/qgis_svg
      # Mounted in SCP and to Hugo for rendering static pages
      - scp_hugo_data:/data/hugo_data
      # Mounted in SCP and to ODM for processing uploaded images
      - scp_odm_data:/data/odm_data
      # Mounted in SCP only for data sharing
      - scp_general_data:/data/general_data
    networks:
      - os-gis-stack  

  ############################################################
  #
  # GeoNode related services
  #
  ############################################################


  geonode:
    profiles: ["all","production","dev"]
    image: *geonode-image
    restart: on-failure
    env_file:
      - .env-geonode
      - .env
    volumes:
      - geonode-static-files:/mnt/volumes/statics
      # This mount is needed when initialising GeoNode
      # so that it can get/set oauth details in the Geoserver 
      # See https://github.com/GeoNode/geonode/blob/master/tasks.py#L216
      - geoserver_data:/geoserver_data/data
      - geonode-backup-restore:/backup_restore
      - geonode-data:/data
      - geonode-tmp:/tmp    
      # We need to override the one shipped by GeoNode
      # because it does not enable SSL and makes some 
      # assumptions about database names and user names 
      # that don't match our configs
      - ./geonode_conf/wait-for-databases:/usr/bin/wait-for-databases
    healthcheck:
      test: "curl --fail --silent --write-out 'HTTP CODE : %{http_code}\n' --output /dev/null http://127.0.0.1:8000/"
      interval: 60s
      timeout: 10s
      retries: 1
      start_period: 60s
    depends_on:
      - geoserver
    environment:
      - IS_CELERY=False
      # See this link for explanation of URL schemes below:
      #  https://github.com/kennethreitz/dj-database-url#url-schema
      # This is the database that geonode stores its content in
      - DATABASE_URL=postgres://${GEONODE_DATABASE_USER}:${GEONODE_DATABASE_PASSWORD}@db:5432/${GEONODE_DATABASE}
      # This is the database that GeoServer uses of its data for stuff uploaded in geonode
      - GEODATABASE_URL=postgis://${GEONODE_GEODATABASE_USER}:${GEONODE_GEODATABASE_PASSWORD}@db:5432/${GEONODE_GEODATABASE}
    entrypoint: ["/usr/src/geonode/entrypoint.sh"]
    command: "uwsgi --ini /usr/src/geonode/uwsgi.ini"
    networks:
      - os-gis-stack  

  # Celery worker that executes celery tasks created by Django.
  geonode-celery:
    profiles: ["all","production","dev"]
    image: *geonode-image
    restart: on-failure
    env_file:
      - .env-geonode
      - .env      
    volumes:
      - geonode-static-files:/mnt/volumes/statics
      #- geonode-geoserver-data:/geoserver_data/data
      - geonode-backup-restore:/backup_restore
      - geonode-data:/data
      - geonode-tmp:/tmp
    depends_on:
      - geonode
    environment:
      - IS_CELERY=True
      # See this link for explanation of URL schemes below:
      #  https://github.com/kennethreitz/dj-database-url#url-schema
      # This is the database that geonode stores its content in
      - DATABASE_URL=postgres://${GEONODE_DATABASE_USER}:${GEONODE_DATABASE_PASSWORD}@db:5432/${GEONODE_DATABASE}
      # This is hte database that GeoServer uses of its data for stuff uploaded in geonode
      - GEODATABASE_URL=postgis://${GEONODE_GEODATABASE_USER}:${GEONODE_GEODATABASE_PASSWORD}@db:5432/${GEONODE_GEODATABASE}      
    entrypoint: ["/usr/src/geonode/entrypoint.sh"]
    command: "celery-cmd"
    networks:
      - os-gis-stack 


  ############################################################
  #
  # MERGIN Service - this is not run in production by
  # default. It will sync your mergin projects to the db image.
  # To run do:
  #
  # docker-compose --profile=mergin up -d
  #
  ############################################################

  mergin-sync:
    image: *mergin-db-sync-image
    profiles: ["all","mergin","dev"]
    volumes:
      # We do this so we can easily inspect what is being pulled
      # down from mergin, flush the working dir when needed etc
      - mergin_sync_data:/tmp
    env_file: .env
    environment:
      - DB_CONN_INFO=host=db dbname=gis user=${POSTGRES_USER} password=${POSTGRES_PASSWORD}
      - DB_SCHEMA_MODIFIED=${DB_SCHEMA_MODIFIED}
      - DB_SCHEMA_BASE=${DB_SCHEMA_BASE}
      - MERGIN_USERNAME=${MERGIN_USER}
      - MERGIN_PASSWORD=${MERGIN_PASSWORD}
      - MERGIN_PROJECT_NAME=${MERGIN_PROJECT_NAME}
      - MERGIN_SYNC_FILE=${MERGIN_SYNC_FILE}
    # Ultimately we want to swap to having the db as the master
    # copy. If you are using the geopackage as the master copy:
    entrypoint: "python3 dbsync_daemon.py --init-from-gpkg"
    # If you are using the database as the master copy:
    #entrypoint: "python3 dbsync_daemon.py --init-from-db"
    networks:
     - os-gis-stack
    restart: unless-stopped
    depends_on:
      - db 

  ##############################################################
  # Will just check out a project and update every interval
  ##############################################################
  mergin-client:
    image: *mergin-client-image
    profiles: ["mergin-client"]
    env_file: .env
    environment:
      - MERGIN_URL=${MERGIN_URL}
      - MERGIN_USER=${MERGIN_USER}
      - MERGIN_PASSWORD=${MERGIN_PASSWORD}
    command: "download-shared"
    #command: "/bin/sh -c 'set -x; mergin.sh download $MERGIN_PROJECT; while :; do sleep 1m & wait $${!}; set -x; cd $MERGIN_PROJECT; mergin pull; cd ..; done' "
    volumes:
      # Host mounted volume
      # Mounted in SCP and to QGIS Server for serving QGIS projects
      - qgis_projects:/home/mergin
    networks:
     - os-gis-stack

  ############################################################
  #
  # OSM Sync Services - these are not run in production by
  # default - to run them do:
  #
  # docker-compose --profile=osm up -d
  #
  ############################################################


  # This is a storage container for the country pbf
  pbf:
    image: pbf:stable
    profiles: ["all","osm","dev"]
    build:
      context: pbf_fetcher
      dockerfile: Dockerfile
    entrypoint: /bin/cp
    command: "/settings/country.pbf /home/settings/country.pbf"
    volumes:
      - ./osm_conf:/home/settings

  imposm:
    image: *imposm-image
    profiles: ["all", "osm","dev"]
    volumes:
      # These are sharable to other containers
      - ./osm_conf:/home/settings
      - osm_import_done:/home/import_done
      - osm_import_queue:/home/import_queue
      - osm_cache:/home/cache
    depends_on:
      - db
    networks:
     - os-gis-stack
    env_file: .env
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASS=${POSTGRES_PASSWORD}
      - POSTGRES_DBNAME=gis
      - POSTGRES_PORT=5432
      - POSTGRES_HOST=db
      # seconds between 2 executions of the script if 0, then no
      # update will be done, only the first initial import from the
      # PBF
      - TIME=120
      # folder for settings (with *.json and *.sql)
      - SETTINGS=settings
      # folder for caching
      - CACHE=cache
      # folder for diff which has been imported
      - IMPORT_DONE=import_done
      # folder for diff which hasn't been imported yet
      - IMPORT_QUEUE=import_queue
      # it can be 3857
      - SRID=4326
      # see http://imposm.org/docs/imposm3/latest/tutorial.html#optimize
      - OPTIMIZE=false
      # see http://imposm.org/docs/imposm3/latest/tutorial.html#deploy-production-tables
      - DBSCHEMA_PRODUCTION=osm
      # http://imposm.org/docs/imposm3/latest/tutorial.html#deploy-production-tables
      - DBSCHEMA_IMPORT=osm_import
      # http://imposm.org/docs/imposm3/latest/tutorial.html#deploy-production-tables
      - DBSCHEMA_BACKUP=osm_backup
      # Install some styles if you are using the default mapping.
      # It can be 'yes' or 'no'
      - QGIS_STYLE=yes
      # Use clip in the database
      - CLIP=yes

  osmupdate:
    image: *osm-update-image
    profiles: ["all","osm","dev"]
  
    volumes:
      # These are sharable to other containers
      - ./osm_conf:/home/settings
      - osm_import_done:/home/import_done
      - osm_import_queue:/home/import_queue
      - osm_cache:/home/cache
    depends_on:
      - db
    networks:
     - os-gis-stack
    environment:
      # These are all currently the defaults but listed here for
      # your convenience if you want to change them.
      
      # The maximum time range to assemble a cumulated changefile.
      - MAX_DAYS=100
      # osmupdate uses a combination of minutely, hourly and daily
      # changefiles. This value can be minute, hour, day or
      # sporadic.
      - DIFF=sporadic
      # argument to determine the maximum number of parallely
      # processed changefiles.
      - MAX_MERGE=7
      # define level for gzip compression. values between 1 (low
      # compression but fast) and 9 (high compression but slow)
      - COMPRESSION_LEVEL=1
      # change the URL to use a custom URL to fetch regional file
      # updates.
      - BASE_URL=http://planet.openstreetmap.org/replication/
      # folder for diff which hasn't been imported yet
      - IMPORT_QUEUE=import_queue
      # folder for diff which has been imported
      - IMPORT_DONE=import_done
      # seconds between 2 executions of the script
      # if 0, then no update will be done, only the first initial
      # import from the PBF
      - TIME=120

  osmenrich:
    image: *osm-enrich-image
    profiles: ["all", "osm","dev"]
    volumes:
      # These are sharable to other containers
      - ./osm_conf:/home/settings
      - osm_import_done:/home/import_done
      - osm_import_queue:/home/import_queue
      - osm_cache:/home/cache
    depends_on:
      - db
    networks:
     - os-gis-stack
    env_file: .env
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASS=${POSTGRES_PASSWORD}
      - POSTGRES_DBNAME=gis
      - POSTGRES_PORT=5432
      - POSTGRES_HOST=db
      # These are all currently the defaults but listed here for
      # your convenience if you want to change them.
      # folder for diff which hasn't been imported yet
      # - IMPORT_QUEUE=import_queue
      # folder for diff which has been imported
      # - IMPORT_DONE=import_done
      # seconds between 2 executions of the script
      # if 0, then no update will be done, only the first initial import from the PBF
      # - TIME=120
      # command: bash -c "while [ ! -f /home/settings/importer.lock ] ; do sleep 1; done && python3 -u /home/enrich.py"


  ############################################################
  #
  #
  # DEV profile. Mainly runs nginx without cert for local testing
  #
  #
  ############################################################

  # Nginx without ssl meant for local testing only      
  nginx-testing:
    image: *nginx-image
    profiles: ["dev"]
    volumes:
     - ./html:/var/www/html
     - ./nginx_dev_conf/nginx.conf:/etc/nginx/nginx.conf
     # Mount the mergin db sync data into nginx so we can access
     # images etc.
     # Ideally we should only be mounting the image subfolders to
     # avoid leaking our geopackage etc.
     # To do that the mergin project needs to be set up accordingly
     # - TODO for the future
     - mergin_sync_data:/mergin-data
    ports:
     - "80:80"
    networks:
     - os-gis-stack
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: 200m
        max-file: '10'
    depends_on:
      - qgis-server
      - mapproxy
      - geoserver
      - postgrest
      - swagger


  ############################################################
  #
  #
  # Mergin client services. These are all run once containers.
  # 
  # The idea is to checkout the projects that have been shared
  # with the designated user into a directory and then publish
  # those using QGIS Server (or GeoServer if you like.).scp_qgis_projects
  #
  #
  ############################################################
    


  download-project:
    image: *mergin-client-image
    profiles: ["mergin-client"]
    env_file: .env
    environment:
      - MERGIN_URL=${MERGIN_URL}
      - MERGIN_USER=${MERGIN_USER}
      - MERGIN_PASSWORD=${MERGIN_PASSWORD}
    command: "download tim1/GloriousProjectOfMagnificence /home/mergin/project"
      #command: "/bin/sh -c 'set -x; mergin.sh download $MERGIN_PROJECT; while :; do sleep 1m & wait $${!}; set -x; cd $MERGIN_PROJECT; mergin pull; cd ..; done' "
    volumes:
      - qgis_projects:/home/mergin

  pull-projects:
    image: *mergin-client-image
    profiles: ["mergin-client"]
    env_file: .env
    environment:
      - MERGIN_URL=${MERGIN_URL}
      - MERGIN_USER=${MERGIN_USER}
      - MERGIN_PASSWORD=${MERGIN_PASSWORD}
    command: "pull"
    volumes:
      - qgis_projects:/home/mergin

  list-shared-projects:
    image: *mergin-client-image
    profiles: ["mergin-client"]
    env_file: .env
    environment:
      - MERGIN_URL=${MERGIN_URL}
      - MERGIN_USER=${MERGIN_USER}
      - MERGIN_PASSWORD=${MERGIN_PASSWORD}
    command: "list-projects --flag shared"
    volumes:
      - qgis_projects:/home/mergin

  download-shared-projects:
    image: *mergin-client-image
    profiles: ["mergin-client"]
    env_file: .env
    environment:
      - MERGIN_URL=${MERGIN_URL}
      - MERGIN_USER=${MERGIN_USER}
      - MERGIN_PASSWORD=${MERGIN_PASSWORD}
    command: "download-shared"
    volumes:
      - qgis_projects:/home/mergin

  ############################################################
  #
  #
  # ODM Services. These are all run once containers.
  #
  #
  ############################################################
    
  odm:
    # A run once container that will process a batch of images and
    # generate an orthophoto, DSM and DEM
    # see https://github.com/OpenDroneMap/ODM#quickstart
    # docker run -ti --rm -v /home/youruser/datasets:/datasets opendronemap/odm --project-path /datasets project
    # Treat this as a run-once job and run it using either
    # docker-compose --profile=odm run odm
    # or 
    # make odm
    image: *odm-image
    # Only runs when you do docker-compose --profile=odm imposm
    profiles: ["odm"]
    volumes:
      # odm datasets should be arranged with project folders each
      # containing an images folder e.g.
      # datasets/smallholding/images
      - scp_odm_data:/datasets
    restart: never
    entrypoint: "python3 /code/run.py --project-path /datasets --dsm --dtm --orthophoto-resolution 2 smallholding"

  odm-ortho-clip:
    
    # A run once container that will clip processed dsm, dtm,
    # orthophoto images created using the odm service above
    # see https://github.com/geo-data/gdal-docker
    # example usage : 
    # docker run -v $(pwd):/data geodata/gdal gdalinfo test.tif
    #
    # Treat this as a run-once job and run it using either
    # docker-compose --profile=odm run odm-ortho-clip
    # or 
    # make odm
    #
    # I think we can use image: kartoza/postgis:13.0 rather and
    # then we get pg support too
    image: *gdal-image
    # Only runs when you do docker-compose --profile=odm .....
    profiles: ["odm"]
    volumes:
      # odm datasets should be arranged with project folders each
      # containing an images folder e.g.
      # datasets/projectfoo/images
      - scp_odm_data:/datasets
      - odm_cutline:/cutline
    restart: never
    entrypoint: "gdalwarp -overwrite -cutline /cutline/cutline.shp -crop_to_cutline -dstalpha /datasets/smallholding/odm_orthophoto/odm_orthophoto.tif /datasets/orthophoto.tif"

  odm-dsm-clip:
    
    # A run once container that will clip processed dsm, dtm,
    # orthophoto images created using the odm service above
    # see https://github.com/geo-data/gdal-docker
    # example usage : 
    # docker run -v $(pwd):/data geodata/gdal gdalinfo test.tif
    #
    # Treat this as a run-once job and run it using either
    # docker-compose --profile=odm run odm-dsm-clip
    # or 
    # make odm
    #
    # I think we can use image: kartoza/postgis:13.0 rather and
    # then we get pg support too    
    image: *gdal-image
    # Only runs when you do docker-compose --profile=odm .....
    profiles: ["odm"]
    volumes:
      # odm datasets should be arranged with project folders each
      # containing an images folder e.g.
      # datasets/projectfoo/images
      - odm_datasets:/datasets
      - odm_cutline:/cutline
    restart: never
    entrypoint: "gdalwarp -overwrite -cutline /cutline/cutline.shp -crop_to_cutline -dstalpha /datasets/smallholding/odm_dem/dsm.tif /datasets/dsm.tif"

  odm-dtm-clip:
    
    # A run once container that will clip processed dsm, dtm,
    # orthophoto images created using the odm service above
    # see https://github.com/geo-data/gdal-docker
    # example usage : 
    # docker run -v $(pwd):/data geodata/gdal gdalinfo test.tif
    #
    # Treat this as a run-once job and run it using either
    # docker-compose --profile=odm run odm-dem-clip
    # or 
    # make odm
    #
    # I think we can use image: kartoza/postgis:13.0 rather and
    # then we get pg support too    
    image: *gdal-image
    # Only runs when you do docker-compose --profile=odm .....
    profiles: ["odm"]
    volumes:
      # odm datasets should be arranged with project folders each
      # containing an images folder e.g.
      # datasets/projectfoo/images
      - odm_datasets:/datasets
      - odm_cutline:/cutline
    restart: never
    entrypoint: "gdalwarp -overwrite -cutline /cutline/cutline.shp -crop_to_cutline -dstalpha /datasets/smallholding/odm_dem/dtm.tif /datasets/dtm.tif"

  osm-to-mbtiles:
    # A run once container that export the docker-osm schema to an
    # mbtiles store
    # Treat this as a run-once job and run it using either
    # docker-compose --profile osm run osm-to-mbtiles
    # or 
    # make osm-to-mbtiles
    #
    # Broken for now, see Makefile osm-to-mbtiles target for
    # details
    image: *postgres-image
    profiles: ["osm-tiles"]
    networks:
      - os-gis-stack
    volumes:
      # This is a bit naughty as the mergindb sync checkout should
      # be treated read only, but trying anyway to see if it works
      # because if it does it could be a nice way to provision an 
      # updated tileset whenever you run this
      - mergin_sync_data:/data
    restart: never
    entrypoint: "ogr2ogr -f MBTILES /data/osm.mbtiles PG:\"dbname='gis' host='db' port='5432' user='docker' password='docker' SCHEMAS=osm\" -dsco \"MAXZOOM=10\" -dsco \"BOUNDS=-7.389126,39.410085,-7.381439,39.415144\""



networks:
  os-gis-stack:
